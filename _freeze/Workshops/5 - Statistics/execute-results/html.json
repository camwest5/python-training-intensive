{
  "hash": "da7572c98cf3a166c54aefad10183e2b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Statistics\n--- \n\nThis session is aimed as an overview of how to perform some statistical modelling with Python. **It is a Python workshop, not a statistics workshop** - if you'd like to better understand the statistical models, or need help deciding what's best for you, please consult a statistics resource or contact a statistician.\n\nIn this session, we'll cover\n\n- Descriptive statistics\n  - Measures of central tendancy\n  - Measures of variability\n  - Measures of correlation\n  \n- Inferential statistics\n  - Linear regressions\n  - Calculating confidence intervals\n  - T-tests\n  - $\\chi^2$ test\n  - ANOVAs\n\nWe'll use three new modules:\n - `numpy`\n - `scipy.stats`\n - `statsmodels`\n\n## Setting up\n\nTo begin, create a new file for this session.\n\n:::{.callout-warning}\n# Be careful with the file name\n\nWe've encountered issues when calling this file *statistics.py*, so please choose a **different name**. Examples include,\n\n- `stats.py`\n- `stats_workshop.py`\n- `statistics_workshop.py`\n\nSpecifically, when you render Quarto files with seaborn plots, it gets confused and uses *your file* to compute statistics instead of its dependency.\n:::\n\nWe'll be working from our \"Players2024\" dataset again. To bring it in and clean it up,\n\n::: {#fc743ed0 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\n\ndf = pd.read_csv(\"data/Players2024.csv\")\ndf = df[df[\"positions\"] != \"Missing\"]\ndf = df[df[\"height_cm\"] > 100]\n```\n:::\n\n\n\n\n## Descriptive Statistics\n\nWe'll start with sample size. All dataframes have most descriptive statistics functions available right off the bat which we access via the `.` operator. \n\nTo calculate the number of non-empty observations in a column, say the numeric variable `df[\"height_cm\"]`, we use the `.count()` method\n\n::: {#5e90cb4c .cell execution_count=3}\n``` {.python .cell-code}\ndf[\"height_cm\"].count()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\n5932\n```\n:::\n:::\n\n\n### Measures of central tendancy\nWe can compute measures of central tendancy similarly. The average value is given by\n\n::: {#1881c46b .cell execution_count=4}\n``` {.python .cell-code}\ndf[\"height_cm\"].mean()\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n183.04130141604855\n```\n:::\n:::\n\n\nthe median by\n\n::: {#54c4e822 .cell execution_count=5}\n``` {.python .cell-code}\ndf[\"height_cm\"].median()\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n183.0\n```\n:::\n:::\n\n\nand the mode by\n\n::: {#ff4d8ea9 .cell execution_count=6}\n``` {.python .cell-code}\ndf[\"height_cm\"].mode()\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n0    185.0\nName: height_cm, dtype: float64\n```\n:::\n:::\n\n\n> `.mode()` returns a dataframe with the most frequent values as there can be multiple.\n\n\n### Measures of variance\n\nWe can also compute measures of variance. The minimum and maximum are as expected\n\n::: {#aea1348e .cell execution_count=7}\n``` {.python .cell-code}\ndf[\"height_cm\"].min()\ndf[\"height_cm\"].max()\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n206.0\n```\n:::\n:::\n\n\nThe range is the difference\n\n::: {#6fdbd175 .cell execution_count=8}\n``` {.python .cell-code}\ndf[\"height_cm\"].min() - df[\"height_cm\"].max()\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n-46.0\n```\n:::\n:::\n\n\nQuantiles are given by `.quantile(...)` with the fraction inside. The inter-quartile range (IQR) is the difference between 25% and 75%.\n\n::: {#f2dbf2fc .cell execution_count=9}\n``` {.python .cell-code}\nq1 = df[\"height_cm\"].quantile(0.25)\nq3 = df[\"height_cm\"].quantile(0.75)\nIQR = q3 - q1\n```\n:::\n\n\nA column's standard deviation and variance are given by\n\n::: {#b23f29f7 .cell execution_count=10}\n``` {.python .cell-code}\ndf[\"height_cm\"].std()\ndf[\"height_cm\"].var()\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n46.7683158241558\n```\n:::\n:::\n\n\nAnd the standard error of the mean (SEM) with\n\n::: {#4f270735 .cell execution_count=11}\n``` {.python .cell-code}\ndf[\"height_cm\"].sem()\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\n0.08879229764682213\n```\n:::\n:::\n\n\nYou can calculate the skewness and kurtosis (variation of tails) of a sample with\n\n::: {#2e708623 .cell execution_count=12}\n``` {.python .cell-code}\ndf[\"height_cm\"].skew()\ndf[\"height_cm\"].kurt()\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\n-0.4338044567190438\n```\n:::\n:::\n\n\nAll together, you can see a nice statistical summary with\n\n::: {#d98381af .cell execution_count=13}\n``` {.python .cell-code}\ndf[\"height_cm\"].describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\ncount    5932.000000\nmean      183.041301\nstd         6.838736\nmin       160.000000\n25%       178.000000\n50%       183.000000\n75%       188.000000\nmax       206.000000\nName: height_cm, dtype: float64\n```\n:::\n:::\n\n\n### Measures of correlation\n\nIf you've got two numeric variables, you might want to examine covariance and correlation. These indicate how strongly the variables are linearly related. We'll need to use the `df[\"age\"]` variable as well.\n\nThe covariance between \"height_cm\" and \"age\" is\n\n::: {#9aaf2d3c .cell execution_count=14}\n``` {.python .cell-code}\ndf[\"height_cm\"].cov(df[\"age\"])\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\n0.5126608276592359\n```\n:::\n:::\n\n\n> The `.cov()` function compares the column it's attached to (here `df[\"height_cm\"]`) with the column you input (here `df[\"age\"]`). This means we could swap the columns without issue:\n>\n> ```{python}\n> df[\"age\"].cov(df[\"height_cm\"])\n> ```\n\nSimilarly, we can find the Pearson correlation coefficient between two columns. \n\n::: {#ece8ef77 .cell execution_count=15}\n``` {.python .cell-code}\ndf[\"height_cm\"].corr(df[\"age\"])\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n0.01682597901197303\n```\n:::\n:::\n\n\nYou can also specify \"kendall\" or \"spearman\" for their respective correlation coefficients\n\n::: {#4a9bfcc5 .cell execution_count=16}\n``` {.python .cell-code}\ndf[\"height_cm\"].corr(df[\"age\"], method = \"kendall\")\ndf[\"height_cm\"].corr(df[\"age\"], method = \"spearman\")\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\n0.007604345289158663\n```\n:::\n:::\n\n\n### Reminder about groupbys\n\nBefore we move to inferential statistics, it's worth reiterating the power of groupbys discussed in the second workshop.\n\nTo group by a specific variable, like \"positions\", we use \n\n::: {#f8ffc621 .cell execution_count=17}\n``` {.python .cell-code}\ngb = df.groupby(\"positions\")\n```\n:::\n\n\nBy applying our statistics to the `gb` object, we'll apply them to *every* variable for *each* position. Note that we should specify `numeric_only = True`, because these statistics won't work for non-numeric variables\n\n::: {#89b66293 .cell execution_count=18}\n``` {.python .cell-code}\ngb.mean(numeric_only = True)\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>height_cm</th>\n      <th>age</th>\n    </tr>\n    <tr>\n      <th>positions</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Attack</th>\n      <td>180.802673</td>\n      <td>25.061108</td>\n    </tr>\n    <tr>\n      <th>Defender</th>\n      <td>184.193269</td>\n      <td>25.716471</td>\n    </tr>\n    <tr>\n      <th>Goalkeeper</th>\n      <td>190.668508</td>\n      <td>26.587017</td>\n    </tr>\n    <tr>\n      <th>Midfield</th>\n      <td>180.497017</td>\n      <td>25.201671</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Inferential Statistics\n\nInferential statistics requires using the module `scipy.stats`, which we'll bring in with\n\n::: {#e6ea866f .cell execution_count=19}\n``` {.python .cell-code}\nimport scipy.stats as stats\n```\n:::\n\n\n### Simple linear regressions\n\nLeast-squares regression for two sets of measurements can be performed with the function `stats.linregress()`\"\n\n::: {#2b8af3ae .cell execution_count=20}\n``` {.python .cell-code}\nstats.linregress(x = df[\"age\"], y = df[\"height_cm\"])\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\nLinregressResult(slope=0.02582749476456191, intercept=182.38260451315895, rvalue=0.01682597901197303, pvalue=0.19506275453364208, stderr=0.01993026652960195, intercept_stderr=0.515991957177263)\n```\n:::\n:::\n\n\nIf we store this as a variable, we can access the different values with the `.` operator. For example, the p-value is\n\n::: {#c625697a .cell execution_count=21}\n``` {.python .cell-code}\nlm = stats.linregress(x = df[\"age\"], y = df[\"height_cm\"])\nlm.pvalue\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\n0.19506275453364208\n```\n:::\n:::\n\n\n#### Plotting it\n\nNaturally, you'd want to plot this. We'll need to use the overlaying techniques from the visualisation session. Let's import **seaborn** and **matplotlib**\n\n::: {#3ab18fac .cell execution_count=22}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n```\n:::\n\n\nStart by making a scatterplot of the data,\n\n::: {#355fbffe .cell execution_count=23}\n``` {.python .cell-code}\nsns.relplot(data = df, x = \"age\", y = \"height_cm\")\n```\n\n::: {.cell-output .cell-output-display}\n![](5 - Statistics_files/figure-html/cell-24-output-1.png){width=470 height=470}\n:::\n:::\n\n\nThen, you'll need to plot the regression as a line. For reference,\n\n$$ y = \\text{slope}\\times x + \\text{intercept}$$\n\nSo\n\n::: {#8a83c680 .cell execution_count=24}\n``` {.python .cell-code}\nsns.relplot(data = df, x = \"age\", y = \"height_cm\")\n\nx_lm = df[\"age\"]\ny_lm = lm.slope*x_lm + lm.intercept\nsns.lineplot(x = x_lm, y = y_lm, color = \"r\")\n```\n\n::: {.cell-output .cell-output-display}\n![](5 - Statistics_files/figure-html/cell-25-output-1.png){width=470 height=470}\n:::\n:::\n\n\n### $t$-tests\n\nWe can also perform $t$-tests with the `scipy.stats` module. Typically, this is performed to examine the statistical signficance of a difference between two samples' means. Let's examine whether that earlier groupby result for is accurate for heights, specifically, **are goalkeepers taller than non-goalkeepers?**\n\nLet's start by separating the goalkeepers from the non-goalkeepers in two variables\n\n::: {#80e2f5ef .cell execution_count=25}\n``` {.python .cell-code}\ngoalkeepers = df[df[\"positions\"] == \"Goalkeeper\"]\nnon_goalkeepers = df[df[\"positions\"] != \"Goalkeeper\"]\n```\n:::\n\n\nThe $t$-test for the means of two independent samples is given by\n\n::: {#b0dfcbcd .cell execution_count=26}\n``` {.python .cell-code}\nstats.ttest_ind(goalkeepers[\"height_cm\"], non_goalkeepers[\"height_cm\"])\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```\nTtestResult(statistic=35.2144964816995, pvalue=7.551647917141636e-247, df=5930.0)\n```\n:::\n:::\n\n\nYielding a p-value of $8\\times 10^{-247}\\approx 0$, indicating that the null-hypothesis (*heights are the same*) is extremely unlikely.\n\n### ANOVAs\n\nWhat about the means of the other three? We could use an ANOVA to examine them. We use the `stats.f_oneway()` function for this. However, this requires us to send a list of samples in for each group, so we should separate the three positions. \n\n::: {#bf9a53d8 .cell execution_count=27}\n``` {.python .cell-code}\ndefender = df[df[\"positions\"] == \"Defender\"].height_cm\nmidfield = df[df[\"positions\"] == \"Midfield\"].height_cm\nattack = df[df[\"positions\"] == \"Attack\"].height_cm\n```\n:::\n\n\nWe can then perform the ANOVA on this list of samples\n\n::: {#09bee644 .cell execution_count=28}\n``` {.python .cell-code}\nstats.f_oneway(defender, midfield, attack)\n```\n\n::: {.cell-output .cell-output-display execution_count=27}\n```\nF_onewayResult(statistic=199.74794987909772, pvalue=2.6216423274516222e-84)\n```\n:::\n:::\n\n\nWith $p = 3\\times10^{-84}$, it looks like their positions are not all independent of height.\n\n### $\\chi^2$ tests\n\n$χ^2$ tests are useful for examining the relationship of categorical variables by comparing the frequencies of each. Often, you'd use this if you can make a contingency table.\n\nWe only have one useful categorical variable here, \"positions\" (the others have too many unique values), so we'll need to create another. Let's see if there's a relationship between players' positions and names with the letter \"a\".\n\nMake a binary column for players with the letter \"a\" in their names. To do this, we need to apply a string method to *all* the columns in the dataframe as follows\n\n::: {#317ff6b8 .cell execution_count=29}\n``` {.python .cell-code}\ndf[\"a_in_name\"] = df[\"name\"].str.contains(\"a\")\n```\n:::\n\n\nLet's cross tabulate positions with this new column\n\n::: {#486bdd11 .cell execution_count=30}\n``` {.python .cell-code}\na_vs_pos = pd.crosstab(df[\"positions\"],df[\"a_in_name\"])\nprint(a_vs_pos)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\na_in_name   False  True \npositions               \nAttack        291   1280\nDefender      355   1606\nGoalkeeper    149    575\nMidfield      312   1364\n```\n:::\n:::\n\n\nThe $χ^2$ test's job is to examine whether players' positions depend on the presence of \"a\" in their name. To evaluate it we need to send the contingency table in:\n\n::: {#b15a904b .cell execution_count=31}\n``` {.python .cell-code}\nstats.chi2_contingency(a_vs_pos)\n```\n\n::: {.cell-output .cell-output-display execution_count=30}\n```\nChi2ContingencyResult(statistic=2.1808405074930404, pvalue=0.5357320466340116, dof=3, expected_freq=array([[ 293.17211733, 1277.82788267],\n       [ 365.9519555 , 1595.0480445 ],\n       [ 135.10923803,  588.89076197],\n       [ 312.76668914, 1363.23331086]]))\n```\n:::\n:::\n\n\n### More complex modelling\n\nIf you need to do more advanced statistics, particularly if you need more regressions, you'll likely need to turn to a different package: `statsmodels`. It is particularly useful for **statistical modelling**.\n\nWe'll go through three examples\n\n1. Simple linear regressions (like before)\n2. Multiple linear regressions\n3. Logistic regressions\n\nWhat's nice about `statsmodels` is that it gives an R-like interface and summaries.\n\nTo start with, let's import the tools. We'll use the *formula* interface, which offers us an R-like way of creating models.\n\n::: {#bb92018d .cell execution_count=32}\n``` {.python .cell-code}\nimport statsmodels.formula.api as smf\n```\n:::\n\n\n#### Simple linear regressions revisited\n\nLet's perform the same linear regression as before, looking at the \"age\" and \"height variables\". Our thinking is that players' heights dictate how long they can play, so we'll make $x = \\text{height\\_cm}$ and $y = \\text{age}$.\n\nThe first step is to make the set up the variables. We'll use the function `smf.ols()` for ordinary least squares. It takes in two imputs:\n\n* The formula string, in the form `y ~ X1 + X2 ...`\n* The data\n\nWe create the model and compute the fit\n\n::: {#badcd254 .cell execution_count=33}\n``` {.python .cell-code}\nmod = smf.ols(\"age ~ height_cm\", df)\nres = mod.fit()\n```\n:::\n\n\nDone! Let's take a look at the results\n\n::: {#e3dab4d8 .cell execution_count=34}\n``` {.python .cell-code}\nres.summary()\n```\n\n::: {.cell-output .cell-output-display execution_count=33}\n```{=html}\n<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>           <td>age</td>       <th>  R-squared:         </th> <td>   0.000</td> \n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.000</td> \n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1.679</td> \n</tr>\n<tr>\n  <th>Date:</th>             <td>Thu, 10 Jul 2025</td> <th>  Prob (F-statistic):</th>  <td> 0.195</td>  \n</tr>\n<tr>\n  <th>Time:</th>                 <td>15:59:56</td>     <th>  Log-Likelihood:    </th> <td> -17279.</td> \n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>  5932</td>      <th>  AIC:               </th> <td>3.456e+04</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>  5930</td>      <th>  BIC:               </th> <td>3.457e+04</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Intercept</th> <td>   23.4973</td> <td>    1.549</td> <td>   15.165</td> <td> 0.000</td> <td>   20.460</td> <td>   26.535</td>\n</tr>\n<tr>\n  <th>height_cm</th> <td>    0.0110</td> <td>    0.008</td> <td>    1.296</td> <td> 0.195</td> <td>   -0.006</td> <td>    0.028</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>204.256</td> <th>  Durbin-Watson:     </th> <td>   0.269</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 206.613</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td> 0.427</td>  <th>  Prob(JB):          </th> <td>1.36e-45</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 2.671</td>  <th>  Cond. No.          </th> <td>4.91e+03</td>\n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 4.91e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems.\n```\n:::\n:::\n\n\n\\\nThat's a lot nicer than with scipy. We can also make a plot by getting the model's $y$ values with `res.fittedvalues`\n\n::: {#3c2ce7c4 .cell execution_count=35}\n``` {.python .cell-code}\nsns.relplot(data = df, x = \"height_cm\", y = \"age\")\nsns.lineplot(x = df[\"height_cm\"], y = res.fittedvalues, color = \"black\")\n```\n\n::: {.cell-output .cell-output-display}\n![](5 - Statistics_files/figure-html/cell-36-output-1.png){width=470 height=470}\n:::\n:::\n\n\n#### Generalised linear models\n\nThe `statsmodels` module has lots of advanced statistical models available. We'll take a look at one more: Generalised Linear Models. The distributions they include are\n\n* Binomial\n* Poisson\n* Negative Binomial\n* Gaussian (Normal)\n* Gamma\n* Inverse Gaussian\n* Tweedie\n\nWe'll use the *binomial* option to create logistic regressions.\n\nLogistic regressions examine the distribution of binary data. For us, we can compare the heights of **goalkeepers vs non-goalkeepers** again. Let's make a new column which is `1` for goalkeepers and `0` for non-goalkeepers:\n\n::: {#a3377824 .cell execution_count=36}\n``` {.python .cell-code}\ndf[\"gk\"] = (df[\"positions\"] == \"Goalkeeper\")*1\n```\n:::\n\n\n> Multiplying by 1 turns `True` $\\rightarrow$ `1` and `False` $\\rightarrow$ `0`\n\nNow, we can model this column with height. Specifically,\n\n$$ \\text{gk} \\sim \\text{height\\_cm}$$\n\nStart by making the model with the function `smf.glm()`. We need to specify the family of distributions; they all live in `sm.families`, which comes from a different submodule that we should import:\n\n::: {#0a56a75b .cell execution_count=37}\n``` {.python .cell-code}\nimport statsmodels.api as sm\nmod = smf.glm(\"gk ~ height_cm\", data = df, family = sm.families.Binomial())\n```\n:::\n\n\nNext, evaluate the results\n\n::: {#3303b06b .cell execution_count=38}\n``` {.python .cell-code}\nres = mod.fit()\n```\n:::\n\n\nLet's have a look at the summary:\n\n::: {#34e6057e .cell execution_count=39}\n``` {.python .cell-code}\nres.summary()\n```\n\n::: {.cell-output .cell-output-display execution_count=38}\n```{=html}\n<table class=\"simpletable\">\n<caption>Generalized Linear Model Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>          <td>gk</td>        <th>  No. Observations:  </th>  <td>  5932</td> \n</tr>\n<tr>\n  <th>Model:</th>                  <td>GLM</td>       <th>  Df Residuals:      </th>  <td>  5930</td> \n</tr>\n<tr>\n  <th>Model Family:</th>        <td>Binomial</td>     <th>  Df Model:          </th>  <td>     1</td> \n</tr>\n<tr>\n  <th>Link Function:</th>         <td>Logit</td>      <th>  Scale:             </th> <td>  1.0000</td>\n</tr>\n<tr>\n  <th>Method:</th>                <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -1583.5</td>\n</tr>\n<tr>\n  <th>Date:</th>            <td>Thu, 10 Jul 2025</td> <th>  Deviance:          </th> <td>  3167.0</td>\n</tr>\n<tr>\n  <th>Time:</th>                <td>15:59:59</td>     <th>  Pearson chi2:      </th> <td>4.02e+03</td>\n</tr>\n<tr>\n  <th>No. Iterations:</th>          <td>7</td>        <th>  Pseudo R-squ. (CS):</th>  <td>0.1879</td> \n</tr>\n<tr>\n  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Intercept</th> <td>  -53.2336</td> <td>    1.927</td> <td>  -27.622</td> <td> 0.000</td> <td>  -57.011</td> <td>  -49.456</td>\n</tr>\n<tr>\n  <th>height_cm</th> <td>    0.2745</td> <td>    0.010</td> <td>   26.938</td> <td> 0.000</td> <td>    0.255</td> <td>    0.294</td>\n</tr>\n</table>\n```\n:::\n:::\n\n\nFinally, we can plot the result like before\n\n::: {#43943411 .cell execution_count=40}\n``` {.python .cell-code}\nsns.relplot(data = df, x = \"height_cm\", y = \"gk\")\nsns.lineplot(x = df[\"height_cm\"], y = res.fittedvalues, color = \"black\")\n```\n\n::: {.cell-output .cell-output-display}\n![](5 - Statistics_files/figure-html/cell-41-output-1.png){width=470 height=470}\n:::\n:::\n\n\n",
    "supporting": [
      "5 - Statistics_files\\figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}